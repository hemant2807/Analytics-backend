ğŸ“Š Analytics Backend â€” Event Ingestion, Queue Processing & Reporting
This project implements a high-performance analytics backend capable of handling large volumes of incoming web events using asynchronous processing, a Redis-backed queue, and Postgres storage, with a clean reporting API for aggregated statistics.
This README explains the architecture, the queue mechanism, database schema, setup steps, and how to use the APIs.

ğŸ§  1. Architecture Decision
ğŸ¯ Goal
Handle extremely high volumes of analytics events without slowing down ingestion.
Your ingestion endpoint must return instantly even under heavy load.
ğŸ—ï¸ Chosen Architecture
Client â†’ Ingestion API â†’ Redis Queue â†’ Worker â†’ Postgres â†’ Reporting API

ğŸŸ¦ Why Asynchronous Processing?
Directly writing each incoming event into a database causes:
Connection bottlenecks
Slow response times
API timeouts during traffic spikes
Using a queue solves this by decoupling ingestion from processing.
ğŸŸ¥ Why Redis + BullMQ?
Redis is in-memory â†’ very fast
BullMQ handles retries, backoff, job persistence
Allows horizontal scaling: multiple workers can process jobs in parallel
ğŸŸ© Benefits
Ingestion is non-blocking
System absorbs traffic spikes safely
Workers can be scaled independently
Durable write processing with retries
Industry-standard architecture (Segment, RudderStack, Mixpanel use similar)

ğŸ—„ï¸ 2. Database Schema
Events are stored in a single table optimized for analytics queries.
ğŸŸ© SQL Schema
```sql
CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
```

ğŸ§± Indexes
```sql
CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));
CREATE INDEX idx_events_path ON events (site_id, path);
```

ğŸ“Š Simple ER Diagram
```
+-------------------------------+
|            events             |
+-------------------------------+
| id (PK)                       |
| site_id                       |
| event_type                    |
| path                          |
| user_id                       |
| timestamp                     |
| created_at                    |
+-------------------------------+
```


ğŸ› ï¸ 3. Setup Instructions (Step-by-Step)
âœ… Prerequisites
Node.js (v18+)
Postgres installed locally
Redis running inside WSL (Ubuntu)
VS Code or any terminal

ğŸ”§ 3.1 Clone or Create the Repository
```
analytics-backend/
â”œâ”€â”€ ingestion/
â”œâ”€â”€ processor/
â”œâ”€â”€ reporting/
â””â”€â”€ shared/   (empty)
```


ğŸ—„ï¸ 3.2 Setup Postgres
Open psql and run:
```sql
CREATE DATABASE analytics;

\c analytics;

CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));
```


ğŸ”Œ 3.3 Start Redis (inside WSL)
```
sudo service redis-server start
redis-cli ping

Expected:
PONG
```


ğŸ“¦ 3.4 Install Dependencies
Ingestion service
```
cd ingestion
npm install
```

Processor service
```
cd processor
npm install
```

Reporting service
```
cd reporting
npm install
```


âš™ï¸ 3.5 Environment Variables
```
ingestion/.env
PORT=3000

processor/.env
DATABASE_URL=postgresql://postgres:YOURPASSWORD@localhost:5432/analytics

reporting/.env
PORT=4000
DATABASE_URL=postgresql://postgres:YOURPASSWORD@localhost:5432/analytics
```


ğŸš€ 3.6 Start All Services (3 Terminals)
Terminal 1 â€” ingestion
```
cd ingestion
npm start
```

Terminal 2 â€” processor
```
cd processor
npm start
```

Terminal 3 â€” reporting
```
cd reporting
npm start
```


ğŸ”Œ 4. API Usage (curl Examples)
ğŸŸ¦ POST /event
Send a tracking event to ingestion:
```bash
curl -X POST http://localhost:3000/event \
  -H "Content-Type: application/json" \
  -d "{\"site_id\":\"demo\",\"event_type\":\"view\",\"path\":\"/\",\"user_id\":\"u1\",\"timestamp\":\"2025-11-14T10:00:00Z\"}"
```

Expected output:
```json
{"status":"ok"}
```

The worker terminal should show:
```
job completed 1
```


ğŸŸ© GET /stats
Fetch analytics summary:
```bash
curl "http://localhost:4000/stats?site_id=demo"
```

Expected output:
```json
{
  "site_id":"demo",
  "date":null,
  "total_views":1,
  "unique_users":1,
  "top_paths":[
    {"path":"/","views":1}
  ]
}
```


ğŸ§© 5. How it Works (Short Summary)
1. Client sends /event â†’ ingestion validates & enqueues job
2. Redis queue stores jobs
3. Worker consumes jobs â†’ inserts into Postgres
4. Reporting API aggregates SQL data and returns stats

ğŸ“Š Analytics Backend â€” Event Ingestion, Queue Processing & Reporting
This project implements a high-performance analytics backend capable of handling large volumes of incoming web events using asynchronous processing, a Redis-backed queue, and Postgres storage, with a clean reporting API for aggregated statistics.
This README explains the architecture, the queue mechanism, database schema, setup steps, and how to use the APIs.

ğŸ§  1. Architecture Decision
ğŸ¯ Goal
Handle extremely high volumes of analytics events without slowing down ingestion.
Your ingestion endpoint must return instantly even under heavy load.
ğŸ—ï¸ Chosen Architecture
Client â†’ Ingestion API â†’ Redis Queue â†’ Worker â†’ Postgres â†’ Reporting API

ğŸŸ¦ Why Asynchronous Processing?
Directly writing each incoming event into a database causes:
Connection bottlenecks
Slow response times
API timeouts during traffic spikes
Using a queue solves this by decoupling ingestion from processing.
ğŸŸ¥ Why Redis + BullMQ?
Redis is in-memory â†’ very fast
BullMQ handles retries, backoff, job persistence
Allows horizontal scaling: multiple workers can process jobs in parallel
ğŸŸ© Benefits
Ingestion is non-blocking
System absorbs traffic spikes safely
Workers can be scaled independently
Durable write processing with retries
Industry-standard architecture (Segment, RudderStack, Mixpanel use similar)

ğŸ—„ï¸ 2. Database Schema
Events are stored in a single table optimized for analytics queries.
ğŸŸ© SQL Schema
```sql
CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
```

ğŸ§± Indexes
```sql
CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));
CREATE INDEX idx_events_path ON events (site_id, path);
```

ğŸ“Š Simple ER Diagram
```
+-------------------------------+
|            events             |
+-------------------------------+
| id (PK)                       |
| site_id                       |
| event_type                    |
| path                          |
| user_id                       |
| timestamp                     |
| created_at                    |
+-------------------------------+
```


ğŸ› ï¸ 3. Setup Instructions (Step-by-Step)
âœ… Prerequisites
Node.js (v18+)
Postgres installed locally
Redis running inside WSL (Ubuntu)
VS Code or any terminal

ğŸ”§ 3.1 Clone or Create the Repository
```
analytics-backend/
â”œâ”€â”€ ingestion/
â”œâ”€â”€ processor/
â”œâ”€â”€ reporting/
â””â”€â”€ shared/   (empty)
```


ğŸ—„ï¸ 3.2 Setup Postgres
Open psql and run:
```sql
CREATE DATABASE analytics;

\c analytics;

CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));
```


ğŸ”Œ 3.3 Start Redis (inside WSL)
```
sudo service redis-server start
redis-cli ping

Expected:
PONG
```


ğŸ“¦ 3.4 Install Dependencies
Ingestion service
```
cd ingestion
npm install
```

Processor service
```
cd processor
npm install
```

Reporting service
```
cd reporting
npm install
```


âš™ï¸ 3.5 Environment Variables
```
ingestion/.env
PORT=3000

processor/.env
DATABASE_URL=postgresql://postgres:YOURPASSWORD@localhost:5432/analytics

reporting/.env
PORT=4000
DATABASE_URL=postgresql://postgres:YOURPASSWORD@localhost:5432/analytics
```


ğŸš€ 3.6 Start All Services (3 Terminals)
Terminal 1 â€” ingestion
```
cd ingestion
npm start
```

Terminal 2 â€” processor
```
cd processor
npm start
```

Terminal 3 â€” reporting
```
cd reporting
npm start
```


ğŸ”Œ 4. API Usage (curl Examples)
ğŸŸ¦ POST /event
Send a tracking event to ingestion:
```bash
curl -X POST http://localhost:3000/event \
  -H "Content-Type: application/json" \
  -d "{\"site_id\":\"demo\",\"event_type\":\"view\",\"path\":\"/\",\"user_id\":\"u1\",\"timestamp\":\"2025-11-14T10:00:00Z\"}"
```

Expected output:
```json
{"status":"ok"}
```

The worker terminal should show:
```
job completed 1
```


ğŸŸ© GET /stats
Fetch analytics summary:
```bash
curl "http://localhost:4000/stats?site_id=demo"
```

Expected output:
```json
{
  "site_id":"demo",
  "date":null,
  "total_views":1,
  "unique_users":1,
  "top_paths":[
    {"path":"/","views":1}
  ]
}
```


ğŸ§© 5. How it Works (Short Summary)
1. Client sends /event â†’ ingestion validates & enqueues job
2. Redis queue stores jobs
3. Worker consumes jobs â†’ inserts into Postgres
4. Reporting API aggregates SQL data and returns stats


ğŸ“Š Analytics Backend â€” Event Ingestion, Queue Processing & Reporting
This project implements a high-performance analytics backend capable of handling large volumes of incoming web events using asynchronous processing, a Redis-backed queue, and Postgres storage, with a clean reporting API for aggregated statistics.
This README explains the architecture, the queue mechanism, database schema, setup steps, and how to use the APIs.

ğŸ§  1. Architecture Decision
ğŸ¯ Goal
Handle extremely high volumes of analytics events without slowing down ingestion.
Your ingestion endpoint must return instantly even under heavy load.
ğŸ—ï¸ Chosen Architecture
Client â†’ Ingestion API â†’ Redis Queue â†’ Worker â†’ Postgres â†’ Reporting API

ğŸŸ¦ Why Asynchronous Processing?
Directly writing each incoming event into a database causes:
Connection bottlenecks
Slow response times
API timeouts during traffic spikes
Using a queue solves this by decoupling ingestion from processing.
ğŸŸ¥ Why Redis + BullMQ?
Redis is in-memory â†’ very fast
BullMQ handles retries, backoff, job persistence
Allows horizontal scaling: multiple workers can process jobs in parallel
ğŸŸ© Benefits
Ingestion is non-blocking
System absorbs traffic spikes safely
Workers can be scaled independently
Durable write processing with retries
Industry-standard architecture (Segment, RudderStack, Mixpanel use similar)

ğŸ—„ï¸ 2. Database Schema
Events are stored in a single table optimized for analytics queries.
ğŸŸ© SQL Schema
```sql
CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
```

ğŸ§± Indexes
```sql
CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));
CREATE INDEX idx_events_path ON events (site_id, path);
```

ğŸ“Š Simple ER Diagram
```
+-------------------------------+
|            events             |
+-------------------------------+
| id (PK)                       |
| site_id                       |
| event_type                    |
| path                          |
| user_id                       |
| timestamp                     |
| created_at                    |
+-------------------------------+
```


ğŸ› ï¸ 3. Setup Instructions (Step-by-Step)
âœ… Prerequisites
Node.js (v18+)
Postgres installed locally
Redis running inside WSL (Ubuntu)
VS Code or any terminal

ğŸ”§ 3.1 Clone or Create the Repository
```
analytics-backend/
â”œâ”€â”€ ingestion/
â”œâ”€â”€ processor/
â”œâ”€â”€ reporting/
â””â”€â”€ shared/   (empty)
```


ğŸ—„ï¸ 3.2 Setup Postgres
Open psql and run:
```sql
CREATE DATABASE analytics;

\c analytics;

CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));
```


ğŸ”Œ 3.3 Start Redis (inside WSL)
```
sudo service redis-server start
redis-cli ping

Expected:
PONG
```


ğŸ“¦ 3.4 Install Dependencies
Ingestion service
```
cd ingestion
npm install
```

Processor service
```
cd processor
npm install
```

Reporting service
```
cd reporting
npm install
```


âš™ï¸ 3.5 Environment Variables
```
ingestion/.env
PORT=3000

processor/.env
DATABASE_URL=postgresql://postgres:YOURPASSWORD@localhost:5432/analytics

reporting/.env
PORT=4000
DATABASE_URL=postgresql://postgres:YOURPASSWORD@localhost:5432/analytics
```


ğŸš€ 3.6 Start All Services (3 Terminals)
Terminal 1 â€” ingestion
```
cd ingestion
npm start
```

Terminal 2 â€” processor
```
cd processor
npm start
```

Terminal 3 â€” reporting
```
cd reporting
npm start
```


ğŸ”Œ 4. API Usage (curl Examples)
ğŸŸ¦ POST /event
Send a tracking event to ingestion:
```bash
curl -X POST http://localhost:3000/event \
  -H "Content-Type: application/json" \
  -d "{\"site_id\":\"demo\",\"event_type\":\"view\",\"path\":\"/\",\"user_id\":\"u1\",\"timestamp\":\"2025-11-14T10:00:00Z\"}"
```

Expected output:
```json
{"status":"ok"}
```

The worker terminal should show:
```
job completed 1
```


ğŸŸ© GET /stats
Fetch analytics summary:
```bash
curl "http://localhost:4000/stats?site_id=demo"
```

Expected output:
```json
{
  "site_id":"demo",
  "date":null,
  "total_views":1,
  "unique_users":1,
  "top_paths":[
    {"path":"/","views":1}
  ]
}
```


ğŸ§© 5. How it Works (Short Summary)
1. Client sends /event â†’ ingestion validates & enqueues job
2. Redis queue stores jobs
3. Worker consumes jobs â†’ inserts into Postgres
4. Reporting API aggregates SQL data and returns stats


ğŸ“Š Analytics Backend â€” Event Ingestion + Queue + Processing + Reporting
A high-performance backend system for collecting website analytics events, processing them asynchronously through a Redis queue, storing them in Postgres, and providing aggregated reporting.

ğŸš€ Architecture Overview
Client â†’ Ingestion API â†’ Redis Queue â†’ Worker â†’ Postgres â†’ Reporting API

Components
Ingestion API (Express): Accepts events quickly and pushes them to Redis.
Queue (Redis + BullMQ): Handles high-volume asynchronous event ingestion.
Worker (Node.js): Consumes events and writes them to the database.
Database (Postgres): Stores raw events.
Reporting API (Express): Returns aggregated analytics.

ğŸ“ Folder Structure
analytics-backend/
â”œâ”€â”€ ingestion/     # POST /event
â”œâ”€â”€ processor/     # Worker saving events to DB
â”œâ”€â”€ reporting/     # GET /stats
â””â”€â”€ shared/        # (empty)


âš™ï¸ Tech Stack
Node.js
Express.js
Redis (via WSL)
BullMQ (queue)
Postgres
Joi validation

ğŸ› ï¸ Setup Instructions
1. Install dependencies
Run inside each folder:
npm install

2. Start Redis (WSL)
sudo service redis-server start
redis-cli ping   # PONG

3. Create Postgres DB
CREATE DATABASE analytics;

CREATE TABLE events (
  id BIGSERIAL PRIMARY KEY,
  site_id TEXT NOT NULL,
  event_type TEXT NOT NULL,
  path TEXT NOT NULL,
  user_id TEXT,
  timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

CREATE INDEX idx_events_site_date ON events (site_id, (DATE(timestamp)));

4. Start the services
Ingestion
cd ingestion
npm start

Processor
cd processor
npm start

Reporting
cd reporting
npm start


ğŸ§ª Testing
Send event
```bash
curl -X POST http://localhost:3000/event \
  -H "Content-Type: application/json" \
  -d "{\"site_id\":\"demo\",\"event_type\":\"view\",\"path\":\"/\",\"user_id\":\"u1\",\"timestamp\":\"2025-11-14T10:00:00Z\"}"
```

Fetch analytics
```bash
curl "http://localhost:4000/stats?site_id=demo"
```

Expected:
```json
{
  "site_id":"demo",
  "total_views":1,
  "unique_users":1,
  "top_paths":[{"path":"/","views":1}]
}
```


ğŸ“ˆ Scalability Notes
Ingestion is non-blocking due to Redis queue
Worker can scale horizontally
Reporting uses indexed SQL queries
System supports high event volume

